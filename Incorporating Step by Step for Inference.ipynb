{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViXp_ca_sUWQ"
      },
      "source": [
        "# Translation with Sequence -> Sequence Model\n",
        "\n",
        "In this task, we will use a Seq2Seq model for machine translation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Import relevant libraries/packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjmTKwB8AJpY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import sentencepiece as spm\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "import os\n",
        "from nltk.translate.bleu_score import corpus_bleu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"CUDA Available:\", torch.cuda.is_available())\n",
        "print(\"Using device:\", torch.cuda.get_device_name(0))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load data into train, val, and test splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtqRIKPVBQ7V"
      },
      "outputs": [],
      "source": [
        "def load_data(src_path, tgt_path, num_samples=None):\n",
        "    with open(src_path, encoding='utf-8') as f_src, \\\n",
        "         open(tgt_path, encoding='utf-8') as f_tgt:\n",
        "        src_lines = f_src.readlines()\n",
        "        tgt_lines = f_tgt.readlines()\n",
        "        assert len(src_lines) == len(tgt_lines), \"Source and target files must have the same number of lines.\"\n",
        "        pairs = [(s.strip(), t.strip())\n",
        "                 for s, t in zip(src_lines, tgt_lines)\n",
        "                 if s.strip() and t.strip()]\n",
        "    return pairs if num_samples is None else pairs[:num_samples]\n",
        "\n",
        "# load everything into lists of (src_str, tgt_str)\n",
        "train_data = load_data(\"./train.en\", \"./train.fr\", num_samples=None)\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.1, random_state=42)\n",
        "test_data  = load_data(\"./test.en\",  \"./test.fr\",  num_samples=None)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRB2aGbC-sv_"
      },
      "source": [
        "#### Using SentencePiece to tokenize and POS tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not os.path.exists(\"./spm_joint.model\"):\n",
        "  spm.SentencePieceTrainer.Train(\n",
        "      input=\"files/JRC-Acquis.English-French.en,files/JRC-Acquis.French-English.fr\",\n",
        "      model_prefix=\"spm_joint\",\n",
        "      vocab_size=16000,\n",
        "      model_type=\"bpe\",\n",
        "      character_coverage=1.0,\n",
        "      bos_id=1, eos_id=2, pad_id=0, unk_id=3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load(\"./spm_joint.model\")\n",
        "\n",
        "# convenience:\n",
        "PAD_ID = sp.pad_id()   # 0\n",
        "BOS_ID = sp.bos_id()   # 1\n",
        "EOS_ID = sp.eos_id()   # 2\n",
        "UNK_ID = sp.unk_id()   # 3\n",
        "VOCAB_SIZE = sp.GetPieceSize()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Showing how the tokenizing step is currently working"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "src_tokens_en = sp.EncodeAsPieces(train_data[2][0], add_bos=True, add_eos=True)\n",
        "src_tokens_fr = sp.EncodeAsPieces(train_data[2][1], add_bos=True, add_eos=True)\n",
        "src_ids_en = [BOS_ID] + sp.EncodeAsIds(train_data[2][0]) + [EOS_ID]\n",
        "src_ids_fr = [BOS_ID] + sp.EncodeAsIds(train_data[2][1]) + [EOS_ID]\n",
        "\n",
        "print(src_tokens_en)\n",
        "print(src_tokens_fr)\n",
        "print(src_ids_en)\n",
        "print(src_ids_fr)\n",
        "print(sp.DecodeIds(src_ids_en))\n",
        "print(sp.DecodeIds(src_ids_fr))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Creating the Translation dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "v37tOO9u-5UG"
      },
      "outputs": [],
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, pairs, sp, max_len=64):\n",
        "        \"\"\"\n",
        "        pairs: list of (src_str, tgt_str)\n",
        "        sp:    loaded spm.SentencePieceProcessor\n",
        "        \"\"\"\n",
        "        self.pairs   = pairs\n",
        "        self.sp      = sp\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src, tgt = self.pairs[idx]\n",
        "\n",
        "        # encode returns a list of IDs (no BOS/EOS by default)\n",
        "        src_ids = [BOS_ID] + self.sp.EncodeAsIds(src)[:self.max_len-2] + [EOS_ID]\n",
        "        tgt_ids = [BOS_ID] + self.sp.EncodeAsIds(tgt)[:self.max_len-2] + [EOS_ID]\n",
        "\n",
        "        return torch.tensor(src_ids, dtype=torch.long), \\\n",
        "               torch.tensor(tgt_ids, dtype=torch.long)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = zip(*batch)\n",
        "    \n",
        "    # pad to fixed length manually\n",
        "    src_padded = pad_sequence(src_batch, batch_first=True, padding_value=PAD_ID)\n",
        "    tgt_padded = pad_sequence(tgt_batch, batch_first=True, padding_value=PAD_ID)\n",
        "    \n",
        "    return src_padded.to(device), tgt_padded.to(device)\n",
        "\n",
        "\n",
        "train_dataset = TranslationDataset(train_data, sp)\n",
        "train_loader  = DataLoader(train_dataset,\n",
        "                           batch_size=32,\n",
        "                           shuffle=True,\n",
        "                           collate_fn=collate_fn)\n",
        "\n",
        "val_dataset   = TranslationDataset(val_data, sp)\n",
        "val_loader    = DataLoader(val_dataset,\n",
        "                           batch_size=32,\n",
        "                           shuffle=False,                   # no need to shuffle at eval time\n",
        "                           collate_fn=collate_fn)\n",
        "\n",
        "test_dataset = TranslationDataset(test_data, sp)            # same BPE dataset class\n",
        "test_loader  = DataLoader(test_dataset,\n",
        "                          batch_size=1,                     # or whatever batch‐size you like\n",
        "                          shuffle=False,                    # no need to shuffle at eval time\n",
        "                          collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Printing samples from DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Source batch shape: torch.Size([32, 64])\n",
            "Target batch shape: torch.Size([32, 64])\n",
            "Source batch example (first row): tensor([    1,    54, 15674, 15694,     9,  8418,   584,  4779,    34,    21,\n",
            "         4317,  2805, 15683,  1782,    53,  4713,  6028, 13543,    21,  3182,\n",
            "           54,   224,  6510,    22,    34,  5766, 15683, 13079,    51,    98,\n",
            "        12223, 15683,  8537,    51,    98,  6954, 15683,  7638, 15686,  5890,\n",
            "            2,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0], device='cuda:0')\n",
            "Target batch example (first row): tensor([    1,    16, 15694,   313,  3522,    84,  2294,  8831,   926,   313,\n",
            "         2449, 10762,  3828,   313,  4713,  3796,    66,    78,  2702,  6628,\n",
            "         5007,    54,   224,  6510,    56, 15683, 11920,  3241, 13425, 15683,\n",
            "         7770,    69,  9400, 15683,  7638, 15146,     2,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0], device='cuda:0')\n",
            "Source batch example (first row) decoded: (c) a summary outline of the proposed measure, including an introduction clearly defining the objectives (diagnosis of problems, targets to be pursued, strategy to be followed, etc.):\n",
            "Target batch example (first row) decoded: c) une présentation du projet comprenant sous une forme synthétique une introduction exposant les objectifs clairement définis (diagnostic, cibles recherchées, stratégie à suivre, etc.),\n"
          ]
        }
      ],
      "source": [
        "# Get the first batch\n",
        "for src_batch, tgt_batch in train_loader:\n",
        "    print(\"Source batch shape:\", src_batch.shape)\n",
        "    print(\"Target batch shape:\", tgt_batch.shape)\n",
        "    print(\"Source batch example (first row):\", src_batch[0])\n",
        "    print(\"Target batch example (first row):\", tgt_batch[0])\n",
        "    break  # stop after first batch\n",
        "\n",
        "print(\"Source batch example (first row) decoded:\", sp.DecodeIds(src_batch[0].tolist()))\n",
        "print(\"Target batch example (first row) decoded:\", sp.DecodeIds(tgt_batch[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "fEJtmRveyxaX"
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(num_embeddings=input_dim, embedding_dim=emb_dim)\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout if n_layers > 1 else 0, batch_first=True, bidirectional=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src):\n",
        "        # src: [batch_size, src_len]        \n",
        "        embedded = self.dropout(self.embedding(src))  # [batch_size, src_len, emb_dim]\n",
        "        \n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "        # outputs: [src_len, batch_size, hid_dim * 2]\n",
        "        # hidden: [n_layers * 2, batch_size, hid_dim]\n",
        "        # cell:   [n_layers * 2, batch_size, hid_dim]\n",
        "        \n",
        "        return outputs, hidden, cell\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Attention Mechanism"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, hid_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.attn = nn.Linear(hid_dim * 4, hid_dim)\n",
        "        self.v = nn.Linear(hid_dim, 1, bias=False)\n",
        "    \n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # hidden: [batch_size, hid_dim * 2]\n",
        "        # encoder_outputs: [batch_size, src_len, hid_dim * 2]\n",
        "        \n",
        "        src_len = encoder_outputs.shape[1]\n",
        "        \n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)  # [batch_size, src_len, hid_dim * 2]\n",
        "        \n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))  # [batch_size, src_len, hid_dim]\n",
        "        attention = self.v(energy).squeeze(2)  # [batch_size, src_len]\n",
        "        \n",
        "        return torch.softmax(attention, dim=1)  # [batch_size, src_len]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout, attention):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.output_dim = output_dim\n",
        "        self.attention = attention\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)        \n",
        "        self.reduce_weighted = nn.Linear(hid_dim * 2, hid_dim)        \n",
        "        self.rnn = nn.LSTM(emb_dim + hid_dim, hid_dim, n_layers, dropout=dropout, batch_first=True)        \n",
        "        self.fc_out = nn.Linear(emb_dim + hid_dim + hid_dim, output_dim)        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, input_token, hidden_cat, hidden, cell, encoder_outputs):\n",
        "        '''\n",
        "        Inference step: processes one token at a time.\n",
        "        input_token: [batch_size, 1]\n",
        "        '''\n",
        "        embedded = self.dropout(self.embedding(input_token))  # [batch_size, 1, emb_dim]\n",
        "\n",
        "        a = self.attention(hidden_cat, encoder_outputs)  # [batch_size, src_len]\n",
        "        a = a.unsqueeze(1)  # [batch_size, 1, src_len]\n",
        "\n",
        "        weighted = torch.bmm(a, encoder_outputs)  # [batch_size, 1, hid_dim*2]\n",
        "        weighted_reduced = self.reduce_weighted(weighted.squeeze(1))  # [batch_size, hid_dim]\n",
        "        weighted_seq = weighted_reduced.unsqueeze(1)  # [batch_size, 1, hid_dim]\n",
        "\n",
        "        rnn_input = torch.cat((embedded, weighted_seq), dim=2)  # [batch_size, 1, emb_dim+hid_dim]\n",
        "\n",
        "        output, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))  # [batch_size, 1, hid_dim]\n",
        "\n",
        "        prediction = self.fc_out(torch.cat((output, weighted_seq, embedded), dim=2))  # [batch_size, 1, output_dim]\n",
        "\n",
        "        return prediction, hidden, cell\n",
        "    \n",
        "    def forward_train(self, input_seq, hidden_cat, hidden, cell, encoder_outputs):\n",
        "            \"\"\"\n",
        "            Training step: processes ENTIRE target sequence\n",
        "            input_seq: [batch_size, seq_len]\n",
        "            \"\"\"\n",
        "            batch_size, seq_len = input_seq.shape\n",
        "            \n",
        "            embedded = self.dropout(self.embedding(input_seq))  # [batch, seq_len, emb_dim]\n",
        "            \n",
        "            a = self.attention(hidden_cat, encoder_outputs)  # [batch, src_len]\n",
        "            a = a.unsqueeze(1)  # [batch, 1, src_len]\n",
        "            \n",
        "            weighted = torch.bmm(a, encoder_outputs)  # [batch, 1, hid_dim*2]\n",
        "            weighted_reduced = self.reduce_weighted(weighted.squeeze(1))  # [batch, hid_dim]\n",
        "            weighted_seq = weighted_reduced.unsqueeze(1).repeat(1, seq_len, 1)  # [batch, seq_len, hid_dim]\n",
        "            \n",
        "            rnn_input = torch.cat((embedded, weighted_seq), dim=2)  # [batch, seq_len, emb_dim + hid_dim]\n",
        "            \n",
        "            output, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))  # [batch, seq_len, hid_dim]\n",
        "            \n",
        "            prediction = self.fc_out(torch.cat((output, weighted_seq, embedded), dim=2))  # [batch, seq_len, output_dim]\n",
        "            \n",
        "            return prediction, hidden, cell\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Seq2Seq wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        batch_size = trg.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
        "\n",
        "        encoder_outputs, hidden, cell = self.encoder(src)\n",
        "\n",
        "        # Reduce bidirectional → unidirectional\n",
        "        hidden = hidden[::2].contiguous()  # [n_layers, batch_size, hid_dim]\n",
        "        cell   = cell[::2].contiguous()      # [n_layers, batch_size, hid_dim]\n",
        "\n",
        "        # For attention: concatenate last forward & backward hidden\n",
        "        # hidden_cat = torch.cat((hidden[-2], hidden[-1]), dim=1)  # [batch_size, hid_dim*2]\n",
        "        hidden_cat = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1) if hidden.shape[0] >= 2 else hidden[-1, :, :]\n",
        "\n",
        "        # 👉 switch to forward_train()\n",
        "        output, hidden, cell = self.decoder.forward_train(trg, hidden_cat, hidden, cell, encoder_outputs)\n",
        "\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Creating the model and loading the saved weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model device: cuda:0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# hyper-parameters\n",
        "INPUT_DIM  = VOCAB_SIZE\n",
        "OUTPUT_DIM = VOCAB_SIZE\n",
        "EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "attn = Attention(HID_DIM)\n",
        "num_epochs = 20\n",
        "\n",
        "encoder = EncoderRNN(INPUT_DIM, EMB_DIM, HID_DIM, 2, dropout=0.3).to(device)\n",
        "decoder = DecoderRNN(OUTPUT_DIM, EMB_DIM, HID_DIM, 2, dropout=0.3, attention=attn).to(device)\n",
        "model = Seq2Seq(encoder, decoder, device).to(device)\n",
        "\n",
        "print(f\"Model device: {next(model.parameters()).device}\")\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID)\n",
        "\n",
        "model.load_state_dict(torch.load(\"seq2seq_model.pth\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## New training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def train(model, iterator, optimizer, criterion, tf_ratio):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for src, tgt in iterator:\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(src, tgt, teacher_forcing_ratio=tf_ratio)\n",
        "        \n",
        "        # flatten, ignore the first token (<sos>)\n",
        "        out = preds[:,1:].reshape(-1, OUTPUT_DIM)\n",
        "        trg = tgt[:,1:].reshape(-1)\n",
        "\n",
        "        loss = criterion(out, trg)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "def evaluate(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, tgt in val_loader:\n",
        "            src = src.to(device)\n",
        "            tgt = tgt.to(device)\n",
        "\n",
        "            output = model(src, tgt)\n",
        "\n",
        "            # skip first token (BOS) for loss calculation\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output[:, 1:].reshape(-1, output_dim)\n",
        "            tgt = tgt[:, 1:].reshape(-1)\n",
        "\n",
        "            loss = criterion(output, tgt)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "    return total_loss / num_batches\n",
        "\n",
        "best_loss = float('inf')\n",
        "patience = 3\n",
        "counter = 0\n",
        "\n",
        "if not os.path.exists(\"./seq2seq_model.pth\"):\n",
        "    for epoch in range(num_epochs):\n",
        "        # linearly decay teacher forcing from 0.9 → 0.5\n",
        "        tf_ratio = max(0.5, 0.9 - 0.02*(epoch-1))\n",
        "        start_time = time.time()\n",
        "        train_loss = train(model, train_loader, optimizer, criterion, tf_ratio)\n",
        "        end_time = time.time()\n",
        "        val_loss = evaluate(model, val_loader, criterion)\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            counter = 0\n",
        "        else:\n",
        "            counter += 1\n",
        "            if counter >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch} with patience {patience}.\")\n",
        "                break\n",
        "        print(f\"Epoch {epoch} | Training Loss {train_loss:.4f} | Validation Loss {val_loss:.4f} | TF={tf_ratio:.2f} | Time: {(end_time-start_time):.2f}s\", flush=True)\n",
        "\n",
        "    torch.save(model.state_dict(), \"seq2seq_model.pth\")\n",
        "\n",
        "# load the model for evaluation\n",
        "model.load_state_dict(torch.load(\"seq2seq_model.pth\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-s-STg90buX"
      },
      "source": [
        "## Evaluate your model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### BLEU score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(model, test_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "    example_shown = False\n",
        "\n",
        "    references = []\n",
        "    hypotheses = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, tgt in test_loader:\n",
        "            output = model(src, tgt)  # shape: [batch_size, trg_len, vocab_size]\n",
        "            loss = criterion(output[:, 1:].reshape(-1, output.shape[-1]), tgt[:, 1:].reshape(-1))\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "            # Get predicted token IDs\n",
        "            predicted_ids = output.argmax(2)[0].tolist()  # take first sample (batch_size=1)\n",
        "\n",
        "            # Strip special tokens\n",
        "            predicted_ids_clean = [i for i in predicted_ids if i not in (PAD_ID, BOS_ID, EOS_ID)]\n",
        "            tgt_ids_clean = [i for i in tgt[0].tolist() if i not in (PAD_ID, BOS_ID, EOS_ID)]\n",
        "\n",
        "            # Decode to sentence (string)\n",
        "            predicted_sentence = sp.DecodeIds(predicted_ids_clean)\n",
        "            tgt_sentence = sp.DecodeIds(tgt_ids_clean)\n",
        "\n",
        "            # For BLEU → tokenize into list of tokens\n",
        "            pred_tokens = predicted_sentence.strip().split()\n",
        "            tgt_tokens = tgt_sentence.strip().split()\n",
        "\n",
        "            hypotheses.append(pred_tokens)\n",
        "            references.append([tgt_tokens])  # reference needs to be a list of refs\n",
        "\n",
        "            if not example_shown:\n",
        "                src_ids_clean = [i for i in src[0].tolist() if i not in (PAD_ID, BOS_ID, EOS_ID)]\n",
        "                src_sentence = sp.DecodeIds(src_ids_clean)\n",
        "                print(f\"English: {src_sentence}\\nTarget French: {tgt_sentence}\\nModel Prediction: {predicted_sentence}\\n\")\n",
        "                example_shown = True\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "    print(f\"Average Test Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Compute BLEU\n",
        "    bleu_score = corpus_bleu(references, hypotheses)\n",
        "    print(f\"Corpus BLEU Score: {bleu_score:.4f}\")\n",
        "\n",
        "evaluate(model, test_loader, criterion)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### New Inference updated for Step by Step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Example 1]\n",
            "English: of 17 March 2005\n",
            "Model Prediction: ________________________________________________________________\n",
            "\n",
            "[Example 2]\n",
            "English: The difference between the spot and forward rates shall be treated as interest payable or receivable.\n",
            "Model Prediction: ________________________________________________________________\n",
            "\n",
            "[Example 3]\n",
            "English: (4) Council Regulation (EC) No 1039/2003 of 2 June 2003 adopting autonomous and transitional measures.\n",
            "Model Prediction: ________________________________________________________________\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def evaluate_model_on_sentences(model, sentences, sp, max_len=64, device=device):\n",
        "    \"\"\"\n",
        "    Run inference on multiple input sentences and print their translations.\n",
        "    \n",
        "    Args:\n",
        "        model: your Seq2Seq model\n",
        "        sentences: list of English source sentences (strings)\n",
        "        sp: loaded SentencePiece tokenizer\n",
        "        max_len: max decoding length\n",
        "        device: computation device\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        # 1️⃣ Encode input\n",
        "        src_ids = [BOS_ID] + sp.EncodeAsIds(sentence)[:max_len-2] + [EOS_ID]\n",
        "        src_tensor = torch.tensor(src_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # 2️⃣ Encoder forward\n",
        "            encoder_outputs, hidden, cell = model.encoder(src_tensor)\n",
        "\n",
        "            # 3️⃣ Reduce bidirectional\n",
        "            hidden = hidden[::2].contiguous()\n",
        "            cell   = cell[::2].contiguous()\n",
        "\n",
        "            # 4️⃣ Concatenate last forward & backward hidden\n",
        "            if hidden.shape[0] >= 2:\n",
        "                hidden_cat = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
        "            else:\n",
        "                hidden_cat = hidden[-1,:,:]\n",
        "\n",
        "            # 5️⃣ Decoder init\n",
        "            input_token = torch.tensor([[BOS_ID]], dtype=torch.long, device=device)\n",
        "\n",
        "            outputs = []\n",
        "            \n",
        "            for _ in range(max_len):\n",
        "                output, hidden, cell = model.decoder(input_token, hidden_cat, hidden, cell, encoder_outputs)\n",
        "                pred_token = output.argmax(2)  # [1,1]\n",
        "                pred_token_id = pred_token.item()\n",
        "\n",
        "                outputs.append(pred_token_id)\n",
        "\n",
        "                if pred_token_id == EOS_ID:\n",
        "                    break\n",
        "\n",
        "                input_token = pred_token\n",
        "\n",
        "        # 6️⃣ Decode token IDs\n",
        "        decoded_sentence = sp.DecodeIds([id for id in outputs if id not in (PAD_ID, BOS_ID, EOS_ID)])\n",
        "\n",
        "        # Optional: clean underscores\n",
        "        # clean_translation = decoded_sentence.replace('_', ' ').strip()\n",
        "\n",
        "        # 7️⃣ Print nicely\n",
        "        print(f\"[Example {i+1}]\")\n",
        "        print(f\"English: {sentence}\")\n",
        "        print(f\"Model Prediction: {decoded_sentence}\")\n",
        "        print()\n",
        "\n",
        "\n",
        "# Example input sentences\n",
        "input_sentences = [\n",
        "    \"of 17 March 2005\",\n",
        "    \"The difference between the spot and forward rates shall be treated as interest payable or receivable.\",\n",
        "    \"(4) Council Regulation (EC) No 1039/2003 of 2 June 2003 adopting autonomous and transitional measures.\"\n",
        "]\n",
        "\n",
        "# Call inference function\n",
        "evaluate_model_on_sentences(model, input_sentences, sp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "_\n"
          ]
        }
      ],
      "source": [
        "print(sp.IdToPiece(15722))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Showing examples of nltk.translate (sentence_bleu & corpus_bleu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
        "\n",
        "# Reference and candidate sentences\n",
        "reference = \"this is a simple example\".split()\n",
        "candidate = \"this is an example\".split()\n",
        "\n",
        "# Calculate BLEU score for a single sentence\n",
        "bleu_score = sentence_bleu([reference], candidate)\n",
        "print(\"BLEU Score (sentence):\", bleu_score)\n",
        "\n",
        "# Calculate BLEU score for a corpus (list of candidate sentences)\n",
        "corpus_reference = [[\"this is a simple example\".split()]]\n",
        "corpus_candidate = [\"this is an example\".split()]\n",
        "\n",
        "corpus_bleu_score = corpus_bleu(corpus_reference, corpus_candidate)\n",
        "print(\"BLEU Score (corpus):\", corpus_bleu_score)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
