{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViXp_ca_sUWQ"
      },
      "source": [
        "# Translation with Sequence -> Sequence Model\n",
        "\n",
        "In this task, we will use a Seq2Seq model for machine translation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Import relevant libraries/packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mjmTKwB8AJpY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import sentencepiece as spm\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "import os\n",
        "from nltk.translate.bleu_score import corpus_bleu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA Available: True\n",
            "Using device: NVIDIA GeForce RTX 4090\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"CUDA Available:\", torch.cuda.is_available())\n",
        "print(\"Using device:\", torch.cuda.get_device_name(0))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VzWZ5OuxS6T"
      },
      "source": [
        "#### RNN for Machine Translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NtqRIKPVBQ7V"
      },
      "outputs": [],
      "source": [
        "def load_data(src_path, tgt_path, num_samples=None):\n",
        "    with open(src_path, encoding='utf-8') as f_src, \\\n",
        "         open(tgt_path, encoding='utf-8') as f_tgt:\n",
        "        src_lines = f_src.readlines()\n",
        "        tgt_lines = f_tgt.readlines()\n",
        "        assert len(src_lines) == len(tgt_lines), \"Source and target files must have the same number of lines.\"\n",
        "        pairs = [(s.strip(), t.strip())\n",
        "                 for s, t in zip(src_lines, tgt_lines)\n",
        "                 if s.strip() and t.strip()]\n",
        "    return pairs if num_samples is None else pairs[:num_samples]\n",
        "\n",
        "# load everything into lists of (src_str, tgt_str)\n",
        "train_data = load_data(\"./train.en\", \"./train.fr\", num_samples=None)\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.1, random_state=42)\n",
        "test_data  = load_data(\"./test.en\",  \"./test.fr\",  num_samples=None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'list'> 659475 <class 'list'> 81417\n",
            "('(4) Figures from the Statistical Office of the European Communities (Eurostat) support the necessity for an increase in all fees by 10 % in order to arrive at the same level of purchasing power as was the case for the fees set in 1998.', \"(4) Les chiffres de l'Office statistique des Communautés européennes (Eurostat) confirment la nécessité de relever toutes les redevances de dix pour cent, afin de retrouver le niveau de pouvoir d'achat assuré par les redevances dont les montants ont été fixés en 1998.\")\n"
          ]
        }
      ],
      "source": [
        "# Showing some information about the data\n",
        "print(type(train_data), len(train_data), type(test_data), len(test_data))\n",
        "print(train_data[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRB2aGbC-sv_"
      },
      "source": [
        "#### Using SentencePiece to tokenize and POS tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not os.path.exists(\"./spm_joint.model\"):\n",
        "  spm.SentencePieceTrainer.Train(\n",
        "      input=\"files/JRC-Acquis.English-French.en,files/JRC-Acquis.French-English.fr\",\n",
        "      model_prefix=\"spm_joint\",\n",
        "      vocab_size=16000,\n",
        "      model_type=\"bpe\",\n",
        "      character_coverage=1.0,\n",
        "      bos_id=1, eos_id=2, pad_id=0, unk_id=3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load(\"./spm_joint.model\")\n",
        "\n",
        "# convenience:\n",
        "PAD_ID = sp.pad_id()   # 0\n",
        "BOS_ID = sp.bos_id()   # 1\n",
        "EOS_ID = sp.eos_id()   # 2\n",
        "UNK_ID = sp.unk_id()   # 3\n",
        "VOCAB_SIZE = sp.GetPieceSize()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Showing how the tokenizing step is currently working"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['<s>', '▁(4)', '▁F', 'ig', 'ures', '▁from', '▁the', '▁Statistical', '▁Office', '▁of', '▁the', '▁European', '▁Communities', '▁(', 'Eurostat', ')', '▁support', '▁the', '▁necess', 'ity', '▁for', '▁an', '▁increase', '▁in', '▁all', '▁fees', '▁by', '▁10', '▁%', '▁in', '▁order', '▁to', '▁arri', 've', '▁at', '▁the', '▁same', '▁level', '▁of', '▁purchasing', '▁power', '▁as', '▁was', '▁the', '▁case', '▁for', '▁the', '▁fees', '▁set', '▁in', '▁1998.', '</s>']\n",
            "['<s>', '▁(4)', '▁Les', '▁chiffres', '▁de', '▁l', \"'\", 'Office', '▁statistique', '▁des', '▁Communautés', '▁européennes', '▁(', 'Eurostat', ')', '▁confir', 'ment', '▁la', '▁nécessité', '▁de', '▁rele', 'ver', '▁toutes', '▁les', '▁redevances', '▁de', '▁dix', '▁pour', '▁cent', ',', '▁afin', '▁de', '▁retro', 'uver', '▁le', '▁niveau', '▁de', '▁pouvoir', '▁d', \"'\", 'achat', '▁assuré', '▁par', '▁les', '▁redevances', '▁dont', '▁les', '▁montants', '▁ont', '▁été', '▁fixés', '▁en', '▁1998.', '</s>']\n",
            "[1, 1138, 355, 155, 239, 344, 21, 12106, 3722, 34, 21, 436, 1786, 54, 9670, 15694, 2383, 21, 789, 167, 99, 53, 3562, 35, 686, 8399, 165, 589, 626, 35, 1091, 51, 5969, 67, 400, 21, 1525, 1263, 34, 15315, 6313, 182, 890, 21, 1099, 99, 21, 8399, 878, 35, 7532, 2]\n",
            "[1, 1138, 357, 7794, 27, 11, 15688, 4015, 4843, 64, 1778, 1761, 54, 9670, 15694, 11577, 46, 43, 5162, 27, 3481, 441, 1658, 78, 9687, 27, 5117, 181, 3097, 15683, 1548, 27, 9747, 1754, 72, 1617, 27, 2756, 6, 15688, 5238, 11287, 70, 78, 9687, 1390, 78, 3542, 782, 614, 4281, 88, 7532, 2]\n",
            "(4) Figures from the Statistical Office of the European Communities (Eurostat) support the necessity for an increase in all fees by 10 % in order to arrive at the same level of purchasing power as was the case for the fees set in 1998.\n",
            "(4) Les chiffres de l'Office statistique des Communautés européennes (Eurostat) confirment la nécessité de relever toutes les redevances de dix pour cent, afin de retrouver le niveau de pouvoir d'achat assuré par les redevances dont les montants ont été fixés en 1998.\n"
          ]
        }
      ],
      "source": [
        "src_tokens_en = sp.EncodeAsPieces(train_data[2][0], add_bos=True, add_eos=True)\n",
        "src_tokens_fr = sp.EncodeAsPieces(train_data[2][1], add_bos=True, add_eos=True)\n",
        "src_ids_en = [BOS_ID] + sp.EncodeAsIds(train_data[2][0]) + [EOS_ID]\n",
        "src_ids_fr = [BOS_ID] + sp.EncodeAsIds(train_data[2][1]) + [EOS_ID]\n",
        "\n",
        "print(src_tokens_en)\n",
        "print(src_tokens_fr)\n",
        "print(src_ids_en)\n",
        "print(src_ids_fr)\n",
        "print(sp.DecodeIds(src_ids_en))\n",
        "print(sp.DecodeIds(src_ids_fr))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Creating the Translation dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "v37tOO9u-5UG"
      },
      "outputs": [],
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, pairs, sp, max_len=64):\n",
        "        \"\"\"\n",
        "        pairs: list of (src_str, tgt_str)\n",
        "        sp:    loaded spm.SentencePieceProcessor\n",
        "        \"\"\"\n",
        "        self.pairs   = pairs\n",
        "        self.sp      = sp\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src, tgt = self.pairs[idx]\n",
        "\n",
        "        # encode returns a list of IDs (no BOS/EOS by default)\n",
        "        src_ids = [BOS_ID] + self.sp.EncodeAsIds(src)[:self.max_len-2] + [EOS_ID]\n",
        "        tgt_ids = [BOS_ID] + self.sp.EncodeAsIds(tgt)[:self.max_len-2] + [EOS_ID]\n",
        "\n",
        "        return torch.tensor(src_ids, dtype=torch.long), \\\n",
        "               torch.tensor(tgt_ids, dtype=torch.long)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = zip(*batch)\n",
        "    \n",
        "    # pad to fixed length manually\n",
        "    src_padded = pad_sequence(src_batch, batch_first=True, padding_value=PAD_ID)\n",
        "    tgt_padded = pad_sequence(tgt_batch, batch_first=True, padding_value=PAD_ID)\n",
        "    \n",
        "    return src_padded.to(device), tgt_padded.to(device)\n",
        "\n",
        "\n",
        "train_dataset = TranslationDataset(train_data, sp)\n",
        "train_loader  = DataLoader(train_dataset,\n",
        "                           batch_size=32,\n",
        "                           shuffle=True,\n",
        "                           collate_fn=collate_fn)\n",
        "\n",
        "val_dataset   = TranslationDataset(val_data, sp)\n",
        "val_loader    = DataLoader(val_dataset,\n",
        "                           batch_size=32,\n",
        "                           shuffle=False,                   # no need to shuffle at eval time\n",
        "                           collate_fn=collate_fn)\n",
        "\n",
        "test_dataset = TranslationDataset(test_data, sp)            # same BPE dataset class\n",
        "test_loader  = DataLoader(test_dataset,\n",
        "                          batch_size=1,                     # or whatever batch‐size you like\n",
        "                          shuffle=False,                    # no need to shuffle at eval time\n",
        "                          collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Printing samples from DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Source batch shape: torch.Size([32, 64])\n",
            "Target batch shape: torch.Size([32, 64])\n",
            "Source batch example (first row): tensor([    1,   658,   864,   214,  1913,    51,  2290,    21,  1661,    34,\n",
            "         6538,   240,   584,    21,  9907, 15688, 15668,  8331, 15683,    74,\n",
            "           35,   622,    21,  5943,    34,    21,  6538,   240,   584,    34,\n",
            "           21,  1659, 11465, 15686,     2,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0], device='cuda:0')\n",
            "Target batch example (first row): tensor([    1,  2587,  3390,   221,  3778,    69,  4327,    78,  2623,     6,\n",
            "        15688, 15505,    64, 11828,    27,    43, 11129, 15683,    76,    88,\n",
            "         2443,    11, 15688, 10311,    27,    11, 15688, 15505,    84,  2260,\n",
            "          245,   194, 15673,   202, 15686,     2,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0], device='cuda:0')\n",
            "Source batch example (first row) decoded: This article is intended to cover the cost of fitting out the Foundation's offices, and in particular the completion of the fitting out of the new building.\n",
            "Target batch example (first row) decoded: Ce crédit est destiné à couvrir les frais d'aménagement des bureaux de la Fondation, et en particulier l'achèvement de l'aménagement du nouvel immeuble.\n"
          ]
        }
      ],
      "source": [
        "# Get the first batch\n",
        "for src_batch, tgt_batch in train_loader:\n",
        "    print(\"Source batch shape:\", src_batch.shape)\n",
        "    print(\"Target batch shape:\", tgt_batch.shape)\n",
        "    print(\"Source batch example (first row):\", src_batch[0])\n",
        "    print(\"Target batch example (first row):\", tgt_batch[0])\n",
        "    break  # stop after first batch\n",
        "\n",
        "print(\"Source batch example (first row) decoded:\", sp.DecodeIds(src_batch[0].tolist()))\n",
        "print(\"Target batch example (first row) decoded:\", sp.DecodeIds(tgt_batch[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fEJtmRveyxaX"
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(num_embeddings=input_dim, embedding_dim=emb_dim)\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout if n_layers > 1 else 0, batch_first=True, bidirectional=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src):\n",
        "        # src: [batch_size, src_len]        \n",
        "        embedded = self.dropout(self.embedding(src))  # [batch_size, src_len, emb_dim]\n",
        "        \n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "        # outputs: [src_len, batch_size, hid_dim * 2]\n",
        "        # hidden: [n_layers * 2, batch_size, hid_dim]\n",
        "        # cell:   [n_layers * 2, batch_size, hid_dim]\n",
        "        \n",
        "        return outputs, hidden, cell\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Attention Mechanism"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, hid_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.attn = nn.Linear(hid_dim * 4, hid_dim)\n",
        "        self.v = nn.Linear(hid_dim, 1, bias=False)\n",
        "    \n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # hidden: [batch_size, hid_dim * 2]\n",
        "        # encoder_outputs: [batch_size, src_len, hid_dim * 2]\n",
        "        \n",
        "        src_len = encoder_outputs.shape[1]\n",
        "        \n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)  # [batch_size, src_len, hid_dim * 2]\n",
        "        \n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))  # [batch_size, src_len, hid_dim]\n",
        "        attention = self.v(energy).squeeze(2)  # [batch_size, src_len]\n",
        "        \n",
        "        return torch.softmax(attention, dim=1)  # [batch_size, src_len]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout, attention):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.output_dim = output_dim\n",
        "        self.attention = attention\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        \n",
        "        self.reduce_weighted = nn.Linear(hid_dim * 2, hid_dim)\n",
        "        \n",
        "        self.rnn = nn.LSTM(emb_dim + hid_dim, hid_dim, n_layers, dropout=dropout, batch_first=True)\n",
        "        \n",
        "        self.fc_out = nn.Linear(emb_dim + hid_dim + hid_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, trg, hidden_cat, hidden, cell, encoder_outputs):\n",
        "        # trg: [batch_size, trg_len]\n",
        "        embedded = self.dropout(self.embedding(trg))  # [batch_size, trg_len, emb_dim]\n",
        "        \n",
        "        batch_size = encoder_outputs.shape[0]\n",
        "        src_len = encoder_outputs.shape[1]\n",
        "        trg_len = embedded.shape[1]\n",
        "        \n",
        "        weighted_list = []\n",
        "        \n",
        "        for t in range(trg_len):\n",
        "            a = self.attention(hidden_cat, encoder_outputs)  # [batch_size, src_len]\n",
        "            a = a.unsqueeze(1)  # [batch_size, 1, src_len]\n",
        "            weighted = torch.bmm(a, encoder_outputs)  # [batch_size, 1, hid_dim*2]\n",
        "            weighted_reduced = self.reduce_weighted(weighted.squeeze(1))  # [batch_size, hid_dim]\n",
        "            weighted_list.append(weighted_reduced.unsqueeze(1))  # [batch_size, 1, hid_dim]\n",
        "        \n",
        "        weighted_seq = torch.cat(weighted_list, dim=1)  # [batch_size, trg_len, hid_dim]\n",
        "        \n",
        "        rnn_input = torch.cat((embedded, weighted_seq), dim=2)  # [batch_size, trg_len, emb_dim + hid_dim]\n",
        "        \n",
        "        output, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))  # output: [batch_size, trg_len, hid_dim]\n",
        "        \n",
        "        # concat output, weighted, embedded\n",
        "        prediction = self.fc_out(torch.cat((output, weighted_seq, embedded), dim=2))  # [batch_size, trg_len, output_dim]\n",
        "        \n",
        "        return prediction, hidden, cell\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Seq2Seq wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        batch_size = trg.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
        "\n",
        "        encoder_outputs, hidden, cell = self.encoder(src)\n",
        "\n",
        "        hidden = hidden[::2].contiguous()  # reduce from 4 layers → 2\n",
        "        cell = cell[::2].contiguous()\n",
        "\n",
        "        hidden_cat = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
        "\n",
        "        # FEED ENTIRE target sequence\n",
        "        output, hidden, cell = self.decoder(trg, hidden_cat, hidden, cell, encoder_outputs)\n",
        "\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Training the Seq2Seq model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model device: cuda:0\n",
            "Epoch 0 | Training Loss 0.0782 | Validation Loss 0.0233 | TF=0.92 | Time: 1661.91s\n",
            "Epoch 1 | Training Loss 0.0402 | Validation Loss 0.0225 | TF=0.90 | Time: 1625.90s\n",
            "Epoch 2 | Training Loss 0.0520 | Validation Loss 0.0226 | TF=0.88 | Time: 1625.31s\n",
            "Epoch 3 | Training Loss 0.0471 | Validation Loss 0.0234 | TF=0.86 | Time: 1625.94s\n",
            "Epoch 4 | Training Loss 0.0652 | Validation Loss 0.0225 | TF=0.84 | Time: 1625.51s\n",
            "Epoch 5 | Training Loss 0.0400 | Validation Loss 0.0235 | TF=0.82 | Time: 1624.89s\n",
            "Epoch 6 | Training Loss 0.0694 | Validation Loss 0.0225 | TF=0.80 | Time: 1624.60s\n",
            "Epoch 7 | Training Loss 0.0505 | Validation Loss 0.0227 | TF=0.78 | Time: 1625.40s\n",
            "Epoch 8 | Training Loss 0.0342 | Validation Loss 0.0236 | TF=0.76 | Time: 1626.53s\n",
            "Early stopping at epoch 9 with patience 3.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# hyper-parameters\n",
        "INPUT_DIM  = VOCAB_SIZE\n",
        "OUTPUT_DIM = VOCAB_SIZE\n",
        "EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "attn = Attention(HID_DIM)\n",
        "num_epochs = 20\n",
        "\n",
        "encoder = EncoderRNN(INPUT_DIM, EMB_DIM, HID_DIM, 2, dropout=0.3).to(device)\n",
        "decoder = DecoderRNN(OUTPUT_DIM, EMB_DIM, HID_DIM, 2, dropout=0.3, attention=attn).to(device)\n",
        "model = Seq2Seq(encoder, decoder, device).to(device)\n",
        "\n",
        "print(f\"Model device: {next(model.parameters()).device}\")\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID)\n",
        "\n",
        "def train(model, iterator, optimizer, criterion, tf_ratio):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for src, tgt in iterator:\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(src, tgt, teacher_forcing_ratio=tf_ratio)\n",
        "        \n",
        "        # flatten, ignore the first token (<sos>)\n",
        "        out = preds[:,1:].reshape(-1, OUTPUT_DIM)\n",
        "        trg = tgt[:,1:].reshape(-1)\n",
        "\n",
        "        loss = criterion(out, trg)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "def evaluate(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, tgt in val_loader:\n",
        "            src = src.to(device)\n",
        "            tgt = tgt.to(device)\n",
        "\n",
        "            output = model(src, tgt)\n",
        "\n",
        "            # skip first token (BOS) for loss calculation\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output[:, 1:].reshape(-1, output_dim)\n",
        "            tgt = tgt[:, 1:].reshape(-1)\n",
        "\n",
        "            loss = criterion(output, tgt)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "    return total_loss / num_batches\n",
        "\n",
        "best_loss = float('inf')\n",
        "patience = 3\n",
        "counter = 0\n",
        "\n",
        "if not os.path.exists(\"./seq2seq_model.pth\"):\n",
        "    for epoch in range(num_epochs):\n",
        "        # linearly decay teacher forcing from 0.9 → 0.5\n",
        "        tf_ratio = max(0.5, 0.9 - 0.02*(epoch-1))\n",
        "        start_time = time.time()\n",
        "        train_loss = train(model, train_loader, optimizer, criterion, tf_ratio)\n",
        "        end_time = time.time()\n",
        "        val_loss = evaluate(model, val_loader, criterion)\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            counter = 0\n",
        "        else:\n",
        "            counter += 1\n",
        "            if counter >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch} with patience {patience}.\")\n",
        "                break\n",
        "        print(f\"Epoch {epoch} | Training Loss {train_loss:.4f} | Validation Loss {val_loss:.4f} | TF={tf_ratio:.2f} | Time: {(end_time-start_time):.2f}s\", flush=True)\n",
        "\n",
        "    torch.save(model.state_dict(), \"seq2seq_model.pth\")\n",
        "\n",
        "# load the model for evaluation\n",
        "model.load_state_dict(torch.load(\"seq2seq_model.pth\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-s-STg90buX"
      },
      "source": [
        "## Evaluate your model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### BLEU score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "English: of 17 March 2005\n",
            "Target French: du 17 mars 2005\n",
            "Model Prediction: _ du 17 mars 2005\n",
            "\n",
            "Average Test Loss: 0.0317\n",
            "Corpus BLEU Score: 0.9379082404544486\n"
          ]
        }
      ],
      "source": [
        "def evaluate(model, test_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "    example_shown = False\n",
        "\n",
        "    references = []\n",
        "    hypotheses = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, tgt in test_loader:\n",
        "            output = model(src, tgt)  # shape: [batch_size, trg_len, vocab_size]\n",
        "            loss = criterion(output[:, 1:].reshape(-1, output.shape[-1]), tgt[:, 1:].reshape(-1))\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "            # Get predicted token IDs\n",
        "            predicted_ids = output.argmax(2)[0].tolist()  # take first sample (batch_size=1)\n",
        "\n",
        "            # Strip special tokens\n",
        "            predicted_ids_clean = [i for i in predicted_ids if i not in (PAD_ID, BOS_ID, EOS_ID)]\n",
        "            tgt_ids_clean = [i for i in tgt[0].tolist() if i not in (PAD_ID, BOS_ID, EOS_ID)]\n",
        "\n",
        "            # Decode to sentence (string)\n",
        "            predicted_sentence = sp.DecodeIds(predicted_ids_clean)\n",
        "            tgt_sentence = sp.DecodeIds(tgt_ids_clean)\n",
        "\n",
        "            # For BLEU → tokenize into list of tokens\n",
        "            pred_tokens = predicted_sentence.strip().split()\n",
        "            tgt_tokens = tgt_sentence.strip().split()\n",
        "\n",
        "            hypotheses.append(pred_tokens)\n",
        "            references.append([tgt_tokens])  # reference needs to be a list of refs\n",
        "\n",
        "            if not example_shown:\n",
        "                src_ids_clean = [i for i in src[0].tolist() if i not in (PAD_ID, BOS_ID, EOS_ID)]\n",
        "                src_sentence = sp.DecodeIds(src_ids_clean)\n",
        "                print(f\"English: {src_sentence}\\nTarget French: {tgt_sentence}\\nModel Prediction: {predicted_sentence}\\n\")\n",
        "                example_shown = True\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "    print(f\"Average Test Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Compute BLEU\n",
        "    bleu_score = corpus_bleu(references, hypotheses)\n",
        "    # print(f\"Corpus BLEU Score: {bleu_score:.4f}\")\n",
        "    print(f\"Corpus BLEU Score: {bleu_score}\")\n",
        "\n",
        "evaluate(model, test_loader, criterion)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Another evaluate function with more examples printed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Example 1]\n",
            "English: of 17 March 2005\n",
            "Target French: du 17 mars 2005\n",
            "Model Prediction: _ du 17 mars 2005\n",
            "\n",
            "[Example 2]\n",
            "English: The difference between the spot and forward rates shall be treated as interest payable or receivable on an accruals basis for both purchases and sales.\n",
            "Target French: La différence entre les cours au comptant et à terme est traitée comme des intérêts à payer ou à recevoir pro rata temporis, pour les achats comme pour les ventes.\n",
            "Model Prediction: _ La différence entre les cours au comptant et à terme est traitée comme des intérêts à payer ou à recevoir pro rata temporis, pour les achats comme pour les ventes.\n",
            "\n",
            "[Example 3]\n",
            "English: 4.\n",
            "Target French: 4.\n",
            "Model Prediction: et 4.\n",
            "\n",
            "[Example 4]\n",
            "English: (4) Council Regulation (EC) No 1039/2003 of 2 June 2003 adopting autonomous and transitional measures concerning the importation of certain processed agricultural products originating in Estonia and the exportation of certain agricultural products to Estonia(7), Council Regulation (EC) No 1086/2003 of 18 June 2003 adopting autonomous and transitional measures concerning the importation of\n",
            "Target French: (4) Le règlement (CE) n° 1039/2003 du Conseil du 2 juin 2003 arrêtant des mesures autonomes et transitoires concernant l'importation de certains produits agricoles transformés originaires de l'Estonie et l'exportation de certains produits agricoles transformés vers l'Estonie(7), le règlement (CE) n° 1086/2003 du\n",
            "Model Prediction: _ (4) Le règlement (CE) n° 1039/2003 du Conseil du 2 juin 2003 arrêtant des mesures autonomes et transitoires concernant l'importation de certains produits agricoles transformés originaires de l'Estonie et l'exportation de certains produits agricoles transformés vers l'Estonie(7), le règlement (CE) n° 1086/2003 du\n",
            "\n",
            "[Example 5]\n",
            "English: Countervailing duties not exceeding the residual countervailing duty imposed in accordance with Article 15(2) of this Regulation may be extended to imports from companies benefiting from individual duties in the countries subject to measures when circumvention of the measures in force is taking place.\n",
            "Target French: En cas de contournement des mesures en vigueur, des droits compensateurs n'excédant pas le droit résiduel institué conformément à l'article 15, paragraphe 2, du présent règlement peuvent être étendus aux importations en provenance de sociétés bénéficiant d'un droit individuel dans les pays soumis aux mesures.\n",
            "Model Prediction: _ En cas de contournement des mesures en vigueur, des droits compensateurs n'excédant pas le droit résiduel institué conformément à l'article 15, paragraphe 2, du présent règlement peuvent être étendus aux importations en provenance de sociétés bénéficiant d'un droit individuel dans les pays soumis aux mesures.\n",
            "\n",
            "[Example 6]\n",
            "English: It shall be updated where appropriate.\n",
            "Target French: Il est mis à jour en fonction des besoins.\n",
            "Model Prediction: _ Il est mis à jour en fonction des besoins.\n",
            "\n",
            "[Example 7]\n",
            "English: (18) The Court of Justice has ruled that, in order to be effective, the principle of equal treatment implies that, whenever it is breached, the compensation awarded to the employee discriminated against must be adequate in relation to the damage sustained.\n",
            "Target French: (18) La Cour de justice a dit pour droit que, pour être effectif, le principe de l'égalité de traitement suppose que, chaque fois qu'il est violé, la réparation accordée au travailleur victime d'une discrimination soit suffisante au regard du préjudice subi.\n",
            "Model Prediction: _ (18) La Cour de justice a dit pour droit que, pour être effectif, le principe de l'égalité de traitement suppose que, chaque fois qu'il est violé, la réparation accordée au travailleur victime d'une discrimination soit suffisante au regard du préjudice subi.\n",
            "\n",
            "[Example 8]\n",
            "English: Article 3\n",
            "Target French: Article 3\n",
            "Model Prediction: _ Article 3\n",
            "\n",
            "[Example 9]\n",
            "English: The terms \"political party\" and \"alliance of political parties\" used in this Regulation should therefore be clarified.\n",
            "Target French: Il convient donc de préciser les notions de \"parti politique\" et d'\"alliance de partis politiques\" qui seront utilisées aux fins du présent règlement.\n",
            "Model Prediction: _ Il convient donc de préciser les notions de \"parti politique\" et d'\"alliance de partis politiques\" qui seront utilisées aux fins du présent règlement.\n",
            "\n",
            "[Example 10]\n",
            "English: The administrative organ shall meet at least once every three months at intervals laid down by the statutes to discuss the progress and foreseeable development of the SE's business.\n",
            "Target French: L'organe d'administration se réunit au moins tous les trois mois selon une périodicité fixée par les statuts pour délibérer de la marche des affaires de la SE et de leur évolution prévisible.\n",
            "Model Prediction: _ L'organe d'administration se réunit au moins tous les trois mois selon une périodicité fixée par les statuts pour délibérer de la marche des affaires de la SE et de leur évolution prévisible.\n",
            "\n",
            "Average Test Loss: 0.0317\n",
            "Corpus BLEU Score: 0.9379082404544486\n"
          ]
        }
      ],
      "source": [
        "def evaluate(model, test_loader, criterion, max_examples=5):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "    example_count = 0  # track how many examples printed\n",
        "\n",
        "    references = []\n",
        "    hypotheses = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, tgt in test_loader:\n",
        "            output = model(src, tgt)  # [batch_size, trg_len, vocab_size]\n",
        "            loss = criterion(output[:, 1:].reshape(-1, output.shape[-1]), tgt[:, 1:].reshape(-1))\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "            predicted_ids_batch = output.argmax(2).tolist()  # [batch_size, trg_len]\n",
        "\n",
        "            for i in range(len(predicted_ids_batch)):\n",
        "                predicted_ids = predicted_ids_batch[i]\n",
        "                tgt_ids = tgt[i].tolist()\n",
        "                src_ids = src[i].tolist()\n",
        "\n",
        "                predicted_ids_clean = [tok for tok in predicted_ids if tok not in (PAD_ID, BOS_ID, EOS_ID)]\n",
        "                tgt_ids_clean = [tok for tok in tgt_ids if tok not in (PAD_ID, BOS_ID, EOS_ID)]\n",
        "                src_ids_clean = [tok for tok in src_ids if tok not in (PAD_ID, BOS_ID, EOS_ID)]\n",
        "\n",
        "                predicted_sentence = sp.DecodeIds(predicted_ids_clean)\n",
        "                tgt_sentence = sp.DecodeIds(tgt_ids_clean)\n",
        "                src_sentence = sp.DecodeIds(src_ids_clean)\n",
        "\n",
        "                pred_tokens = predicted_sentence.strip().split()\n",
        "                tgt_tokens = tgt_sentence.strip().split()\n",
        "\n",
        "                hypotheses.append(pred_tokens)\n",
        "                references.append([tgt_tokens])\n",
        "\n",
        "                # ✅ print up to max_examples\n",
        "                if example_count < max_examples:\n",
        "                    print(f\"[Example {example_count+1}]\")\n",
        "                    print(f\"English: {src_sentence}\")\n",
        "                    print(f\"Target French: {tgt_sentence}\")\n",
        "                    print(f\"Model Prediction: {predicted_sentence}\\n\")\n",
        "                    example_count += 1\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "    print(f\"Average Test Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    bleu_score = corpus_bleu(references, hypotheses)\n",
        "    print(f\"Corpus BLEU Score: {bleu_score}\")\n",
        "\n",
        "# Example: print 10 examples\n",
        "evaluate(model, test_loader, criterion, max_examples=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Inference (test this)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Source sentence: This Regulation shall enter into force on 1 May 2003.\n",
            "Translation IDs: ('________________________________________________________________', [15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722, 15722])\n"
          ]
        }
      ],
      "source": [
        "def translate_sentence(model, sentence, sp, max_len=64, device=device):\n",
        "    model.eval()\n",
        "    \n",
        "    # Encode the sentence using SentencePiece\n",
        "    src_ids = [BOS_ID] + sp.EncodeAsIds(sentence)[:max_len-2] + [EOS_ID]\n",
        "    src_tensor = torch.tensor(src_ids, dtype=torch.long).unsqueeze(0).to(device)  # [1, src_len]\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        # Encoder forward pass\n",
        "        encoder_outputs, hidden, cell = model.encoder(src_tensor)\n",
        "        \n",
        "        # Reduce layers (since bidirectional and n_layers=2 → reduce to 1 layer for decoder)\n",
        "        hidden = hidden[::2].contiguous()  # [n_layers, batch_size, hid_dim]\n",
        "        cell   = cell[::2].contiguous()\n",
        "        \n",
        "        # Create hidden_cat (concatenation of last forward & backward hidden)\n",
        "        hidden_cat = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1) if hidden.shape[0] >= 2 else hidden[-1, :, :]\n",
        "        \n",
        "        # First input to decoder is <bos>\n",
        "        input_token = torch.tensor([[BOS_ID]], dtype=torch.long, device=device)  # [1, 1]\n",
        "        \n",
        "        outputs = []\n",
        "        \n",
        "        for t in range(max_len):\n",
        "            output, hidden, cell = model.decoder(input_token, hidden_cat, hidden, cell, encoder_outputs)\n",
        "            pred_token = output.argmax(2)  # [1, 1]\n",
        "            \n",
        "            pred_token_id = pred_token.item()\n",
        "            outputs.append(pred_token_id)\n",
        "            \n",
        "            if pred_token_id == EOS_ID:\n",
        "                break\n",
        "            \n",
        "            input_token = pred_token  # feed prediction back as next input\n",
        "        \n",
        "    # Decode predicted IDs → tokens\n",
        "    decoded_tokens = sp.DecodeIds([id for id in outputs if id not in (PAD_ID, BOS_ID, EOS_ID)])\n",
        "    \n",
        "    return decoded_tokens, outputs\n",
        "\n",
        "\n",
        "sentence = \"This Regulation shall enter into force on 1 May 2003.\"\n",
        "translation = translate_sentence(model, sentence, sp)\n",
        "print(\"Source sentence:\", sentence)\n",
        "print(\"Translation IDs:\", translation)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
